Feature: Optimization Pipeline Integration
  As a Go developer using go-starter's optimization system
  I want the complete optimization pipeline to work seamlessly
  So that I can optimize real projects with confidence and reliability

  Background:
    Given I am using go-starter CLI
    And the optimization system is available
    And the configuration system is available
    And all templates are properly initialized

  @integration @end-to-end
  Scenario: Complete optimization workflow
    Given I have a real "web-api" project with complex code:
      """
      - Multiple Go files with various patterns
      - Unused imports across different packages
      - Dead code in multiple locations
      - Complex function hierarchies
      - Various architectural patterns
      """
    When I run the complete optimization pipeline:
      | Step                    | Configuration          |
      | Initialize              | Load project structure |
      | Analyze                 | AST analysis           |
      | Plan optimizations      | Generate optimization plan |
      | Apply optimizations     | Execute transformations |
      | Validate results        | Verify code correctness |
      | Generate report         | Performance metrics    |
    Then the pipeline should complete successfully
    And all optimization steps should be executed in order
    And the optimized project should maintain functionality
    And detailed metrics should be available

  @integration @real-projects
  Scenario Outline: Optimize different project types end-to-end
    Given I have a real "<project_type>" project generated by go-starter
    And the project uses "<framework>" framework
    And the project has "<architecture>" architecture
    When I apply "<optimization_level>" optimization through the pipeline
    Then the pipeline should handle the project correctly
    And the project should compile without errors
    And optimization metrics should reflect project characteristics
    And architectural integrity should be maintained

    Examples:
      | project_type | framework | architecture | optimization_level |
      | web-api      | gin       | standard     | safe              |
      | web-api      | echo      | clean        | standard          |
      | web-api      | fiber     | hexagonal    | aggressive        |
      | cli          | cobra     | standard     | standard          |
      | library      | -         | standard     | conservative      |
      | microservice | gin       | ddd          | performance       |

  @integration @error-recovery
  Scenario: Pipeline error handling and recovery
    Given I have a project with problematic Go code:
      """go
      package main
      
      import (
          "fmt"
          "invalid/import/path"
          "github.com/nonexistent/package"
      )
      
      func main() {
          // Syntax error on next line
          fmt.Println("test"
          undefinedFunction()
      }
      """
    When I run the optimization pipeline
    Then the pipeline should detect and handle errors gracefully
    And parsing errors should be reported clearly
    And the pipeline should continue with valid files
    And a comprehensive error report should be generated
    And no files should be corrupted

  @integration @large-projects
  Scenario: Handle large projects efficiently
    Given I have a large project with:
      | Characteristic          | Value    |
      | Total Go files          | 150+     |
      | Lines of code           | 50,000+  |
      | Packages                | 25+      |
      | Unused imports          | 200+     |
      | Unused variables        | 100+     |
      | Unused functions        | 50+      |
    When I optimize with performance settings:
      | Setting                 | Value |
      | MaxConcurrentFiles      | 16    |
      | MaxFileSize             | 2MB   |
      | EnableProgressReporting | true  |
    Then optimization should complete within reasonable time
    And memory usage should remain acceptable
    And progress should be reported accurately
    And all optimizations should be applied correctly

  @integration @concurrent-safety
  Scenario: Concurrent optimization safety
    Given I have multiple projects to optimize:
      | Project     | Type     | Size    |
      | project-a   | web-api  | Medium  |
      | project-b   | cli      | Small   |
      | project-c   | library  | Large   |
    When I run optimization on all projects concurrently
    Then each optimization should complete independently
    And no file corruption should occur
    And resource usage should be properly managed
    And results should be consistent with sequential runs

  @integration @backup-and-restore
  Scenario: Backup and restore functionality
    Given I have a project that I want to optimize safely
    When I enable backup creation and run optimization
    Then backup files should be created for all modified files
    And backup files should contain original content
    When optimization introduces issues
    Then I should be able to restore from backups completely
    And the restored project should match the original exactly

  @integration @dry-run-accuracy
  Scenario: Dry-run accuracy validation
    Given I have a project with known optimization opportunities
    When I run optimization in dry-run mode
    Then the dry-run should report potential changes accurately
    And no actual files should be modified
    When I run the same optimization with dry-run disabled
    Then the actual changes should match the dry-run preview exactly
    And the optimization results should be identical

  @integration @cross-platform
  Scenario: Cross-platform compatibility
    Given I have projects with platform-specific characteristics:
      | Characteristic        | Windows | macOS | Linux |
      | File path separators  | \       | /     | /     |
      | Line endings          | CRLF    | LF    | LF    |
      | Case sensitivity      | No      | No    | Yes   |
    When I run optimization on each platform
    Then the optimization should work correctly on all platforms
    And file paths should be handled properly
    And line endings should be preserved appropriately
    And results should be functionally equivalent

  @integration @memory-management
  Scenario: Memory management during optimization
    Given I have a project that will stress memory usage
    When I monitor memory usage during optimization:
      | Phase              | Expected Memory |
      | Project loading    | < 100MB        |
      | AST parsing        | < 500MB        |
      | Analysis           | < 200MB        |
      | Transformation     | < 300MB        |
      | File writing       | < 150MB        |
    Then memory usage should stay within expected bounds
    And memory should be properly released after each phase
    And garbage collection should be effective

  @integration @plugin-compatibility
  Scenario: Plugin and extension compatibility
    Given I have a project with go-starter plugins:
      | Plugin Type     | Name              | Version |
      | Code generator  | custom-handlers   | 1.2.0   |
      | Template engine | enhanced-layouts  | 2.1.0   |
      | Validator       | strict-types      | 1.0.0   |
    When I run optimization on the plugin-generated code
    Then the optimization should respect plugin-generated patterns
    And plugin functionality should remain intact
    And no plugin-specific code should be broken

  @integration @configuration-integration
  Scenario: Configuration system integration
    Given I have projects with different configuration sources:
      | Project   | Global Config | Project Config | CLI Flags    |
      | project-1 | conservative  | standard       | --level=safe |
      | project-2 | performance   | -              | --dry-run    |
      | project-3 | -             | custom-profile | -            |
    When I optimize each project through the pipeline
    Then configurations should merge correctly for each project
    And the effective configuration should be applied properly
    And configuration precedence should be respected

  @integration @metrics-collection
  Scenario: Comprehensive metrics collection
    Given I have a project suitable for metrics collection
    When I run optimization with metrics enabled
    Then the pipeline should collect comprehensive metrics:
      | Metric Category    | Specific Metrics                          |
      | Performance        | Processing time, files per second         |
      | Code Changes       | Imports removed/added, functions removed  |
      | Quality Impact     | Code size reduction, complexity reduction |
      | Resource Usage     | Memory peak, CPU utilization             |
      | Error Tracking     | Parse errors, optimization failures       |
    And metrics should be accurate and detailed
    And metrics should be exportable in multiple formats

  @integration @rollback-scenarios
  Scenario: Handle rollback scenarios
    Given I have optimized a project successfully
    And later discover optimization introduced subtle bugs
    When I initiate a rollback operation
    Then the rollback should restore the exact original state
    And all optimization changes should be reverted
    And the project should compile and run as before optimization
    And rollback should be atomic (all or nothing)

  @integration @version-compatibility
  Scenario: Version compatibility across go-starter releases
    Given I have projects optimized with different go-starter versions:
      | Project       | go-starter Version | Optimization Version |
      | legacy-v1     | 1.0.0             | 1.0                  |
      | current-v2    | 2.1.0             | 2.1                  |
      | beta-v3       | 3.0.0-beta        | 3.0                  |
    When I run current optimization pipeline on all projects
    Then legacy projects should be handled with backward compatibility
    And optimization metadata should be upgraded appropriately
    And results should be consistent across versions

  @integration @stress-testing
  Scenario: Stress test the optimization pipeline
    Given I have a stress test environment with:
      | Stress Factor       | Value              |
      | Concurrent projects | 10                 |
      | Large files         | 10MB each          |
      | Deep nesting        | 20 levels deep     |
      | Complex imports     | 500+ import statements |
      | Memory pressure     | 80% system memory  |
    When I run optimization under stress conditions
    Then the pipeline should handle stress gracefully
    And performance should degrade gracefully under load
    And no crashes or data corruption should occur
    And error reporting should remain clear and helpful

  @integration @real-world-validation
  Scenario: Real-world project validation
    Given I have actual open-source Go projects:
      | Project Type | Characteristics                    |
      | Web server   | HTTP handlers, middleware, routing |
      | CLI tool     | Commands, flags, file processing   |
      | Library      | Public API, internal utilities     |
      | Service      | gRPC, database, business logic     |
    When I apply optimization to these real projects
    Then optimization should improve code quality measurably
    And no functionality should be broken
    And the projects should pass their existing test suites
    And performance should be maintained or improved